{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Score Prediction: EDA and Modeling\n",
    "\n",
    "This notebook demonstrates the complete workflow for predicting student exam scores:\n",
    "1. Data loading and exploration\n",
    "2. Basic EDA visualizations\n",
    "3. Linear regression baseline\n",
    "4. Polynomial regression with degree selection\n",
    "5. Model comparison and insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our custom modules\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.data import resolve_data_path, load_data, clean_data, split_data\n",
    "from src.features import build_poly\n",
    "from src.modeling import train_linear, compute_metrics, cv_select_poly_degree\n",
    "from src.plots import scatter_xy, pred_vs_actual, residuals\n",
    "from src.config import DEFAULT_DATA_PATH, DEMO_DATA_PATH, DEFAULT_TARGET, DEFAULT_FEATURES\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolve data path (will create demo if needed)\n",
    "data_path = resolve_data_path(DEFAULT_DATA_PATH, DEMO_DATA_PATH)\n",
    "print(f\"Using dataset: {data_path}\")\n",
    "\n",
    "# Load data\n",
    "df = load_data(data_path)\n",
    "print(f\"Dataset loaded successfully: {df.shape[0]} rows, {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about the dataset\n",
    "print(\"Dataset Info:\")\n",
    "print(\"=\" * 40)\n",
    "df.info()\n",
    "\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(\"=\" * 40)\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nBasic statistics:\")\n",
    "print(\"=\" * 40)\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms of key variables\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "columns_to_plot = ['study_hours', 'sleep_hours', 'attendance', 'participation', 'final_score']\n",
    "for i, col in enumerate(columns_to_plot):\n",
    "    if col in df.columns:\n",
    "        axes[i].hist(df[col], bins=20, alpha=0.7, edgecolor='black')\n",
    "        axes[i].set_title(f'{col.replace(\"_\", \" \").title()} Distribution')\n",
    "        axes[i].set_xlabel('Value')\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "# Hide the last subplot if not used\n",
    "if len(columns_to_plot) < len(axes):\n",
    "    axes[-1].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Variable Distributions', fontsize=16, y=1.02)\n",
    "plt.savefig('../outputs/figures/notebook_histograms.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot: Study Hours vs Final Score\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df['study_hours'], df['final_score'], alpha=0.6, edgecolors='black', linewidth=0.5)\n",
    "plt.xlabel('Study Hours')\n",
    "plt.ylabel('Final Score')\n",
    "plt.title('Study Hours vs Final Score')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add trend line\n",
    "z = np.polyfit(df['study_hours'], df['final_score'], 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(df['study_hours'], p(df['study_hours']), \"r--\", alpha=0.8, linewidth=2, label='Trend Line')\n",
    "plt.legend()\n",
    "plt.savefig('../outputs/figures/notebook_scatter.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Calculate correlation\n",
    "correlation = df['study_hours'].corr(df['final_score'])\n",
    "print(f\"Correlation between Study Hours and Final Score: {correlation:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix (numerical only)\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "correlation_matrix = df[numeric_cols].corr()\n",
    "\n",
    "print(\"Correlation Matrix:\")\n",
    "print(\"=\" * 50)\n",
    "print(correlation_matrix.round(3))\n",
    "\n",
    "# Find strongest correlations with final_score\n",
    "if 'final_score' in correlation_matrix.columns:\n",
    "    correlations = correlation_matrix['final_score'].abs().sort_values(ascending=False)\n",
    "    print(f\"\\nStrongest correlations with Final Score:\")\n",
    "    print(\"=\" * 50)\n",
    "    for feature, corr in correlations.items():\n",
    "        if feature != 'final_score':\n",
    "            print(f\"{feature}: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data and split into train/test sets\n",
    "features = ['study_hours']  # Start with single feature\n",
    "target = 'final_score'\n",
    "\n",
    "print(f\"Using features: {features}\")\n",
    "print(f\"Target variable: {target}\")\n",
    "\n",
    "# Clean and split data\n",
    "df_clean = clean_data(df, target, features)\n",
    "X_train, X_test, y_train, y_test = split_data(\n",
    "    df_clean, features, target, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set: {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Linear Regression Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train linear regression model\n",
    "print(\"Training Linear Regression Model...\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "linear_model = train_linear(X_train, y_train)\n",
    "y_pred_linear = linear_model.predict(X_test)\n",
    "\n",
    "# Compute metrics\n",
    "metrics_linear = compute_metrics(y_test, y_pred_linear)\n",
    "\n",
    "print(\"Linear Regression Results:\")\n",
    "for metric, value in metrics_linear.items():\n",
    "    print(f\"{metric.upper()}: {value:.4f}\")\n",
    "\n",
    "print(f\"\\nModel coefficients:\")\n",
    "print(f\"Intercept: {linear_model.intercept_:.4f}\")\n",
    "for i, coef in enumerate(linear_model.coef_):\n",
    "    print(f\"{features[i]}: {coef:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions vs actual for linear model\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Predictions vs Actual\n",
    "ax1.scatter(y_test, y_pred_linear, alpha=0.6, edgecolors='black', linewidth=0.5)\n",
    "min_val = min(y_test.min(), y_pred_linear.min())\n",
    "max_val = max(y_test.max(), y_pred_linear.max())\n",
    "ax1.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')\n",
    "ax1.set_xlabel('Actual Values')\n",
    "ax1.set_ylabel('Predicted Values')\n",
    "ax1.set_title('Linear Regression: Predictions vs Actual')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals\n",
    "residuals_linear = y_test - y_pred_linear\n",
    "ax2.scatter(y_pred_linear, residuals_linear, alpha=0.6, edgecolors='black', linewidth=0.5)\n",
    "ax2.axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "ax2.set_xlabel('Predicted Values')\n",
    "ax2.set_ylabel('Residuals')\n",
    "ax2.set_title('Linear Regression: Residual Plot')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/figures/notebook_linear_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best polynomial degree using cross-validation\n",
    "print(\"Selecting optimal polynomial degree...\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Combine train and test for CV (we'll split again after)\n",
    "X_full = np.vstack([X_train, X_test])\n",
    "y_full = np.concatenate([y_train, y_test])\n",
    "\n",
    "cv_result = cv_select_poly_degree(X_full, y_full, degrees=[2, 3, 4, 5], k=5, random_state=42)\n",
    "best_degree = cv_result['best_degree']\n",
    "\n",
    "print(f\"Cross-validation results:\")\n",
    "for degree, result in cv_result['cv_results'].items():\n",
    "    print(f\"Degree {degree}: RMSE = {result['mean_rmse']:.4f} ± {result['std_rmse']:.4f}\")\n",
    "\n",
    "print(f\"\\nBest degree selected: {best_degree}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train polynomial model with best degree\n",
    "print(f\"\\nTraining Polynomial Regression (degree={best_degree})...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create polynomial features\n",
    "X_train_poly = build_poly(X_train, best_degree)\n",
    "X_test_poly = build_poly(X_test, best_degree)\n",
    "\n",
    "# Train model (linear regression on polynomial features)\n",
    "poly_model = train_linear(X_train_poly, y_train)\n",
    "y_pred_poly = poly_model.predict(X_test_poly)\n",
    "\n",
    "# Compute metrics\n",
    "metrics_poly = compute_metrics(y_test, y_pred_poly)\n",
    "\n",
    "print(f\"Polynomial Regression Results (degree={best_degree}):\")\n",
    "for metric, value in metrics_poly.items():\n",
    "    print(f\"{metric.upper()}: {value:.4f}\")\n",
    "\n",
    "print(f\"\\nNumber of polynomial features: {X_train_poly.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions vs actual for polynomial model\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Predictions vs Actual\n",
    "ax1.scatter(y_test, y_pred_poly, alpha=0.6, edgecolors='black', linewidth=0.5, color='orange')\n",
    "min_val = min(y_test.min(), y_pred_poly.min())\n",
    "max_val = max(y_test.max(), y_pred_poly.max())\n",
    "ax1.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')\n",
    "ax1.set_xlabel('Actual Values')\n",
    "ax1.set_ylabel('Predicted Values')\n",
    "ax1.set_title(f'Polynomial Regression (deg={best_degree}): Predictions vs Actual')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals\n",
    "residuals_poly = y_test - y_pred_poly\n",
    "ax2.scatter(y_pred_poly, residuals_poly, alpha=0.6, edgecolors='black', linewidth=0.5, color='orange')\n",
    "ax2.axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "ax2.set_xlabel('Predicted Values')\n",
    "ax2.set_ylabel('Residuals')\n",
    "ax2.set_title(f'Polynomial Regression (deg={best_degree}): Residual Plot')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/figures/notebook_poly_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare metrics between linear and polynomial models\n",
    "print(\"Model Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Linear': [metrics_linear['mae'], metrics_linear['mse'], metrics_linear['rmse'], metrics_linear['r2']],\n",
    "    'Polynomial': [metrics_poly['mae'], metrics_poly['mse'], metrics_poly['rmse'], metrics_poly['r2']]\n",
    "}, index=['MAE', 'MSE', 'RMSE', 'R²'])\n",
    "\n",
    "print(metrics_df.round(4))\n",
    "\n",
    "# Calculate improvement\n",
    "improvement = {}\n",
    "for metric in ['mae', 'mse', 'rmse']:\n",
    "    # For error metrics, lower is better\n",
    "    improvement[metric] = (metrics_linear[metric] - metrics_poly[metric]) / metrics_linear[metric] * 100\n",
    "\n",
    "# For R², higher is better\n",
    "improvement['r2'] = (metrics_poly['r2'] - metrics_linear['r2']) / metrics_linear['r2'] * 100\n",
    "\n",
    "print(f\"\\nImprovement from Linear to Polynomial:\")\n",
    "for metric, imp in improvement.items():\n",
    "    direction = \"reduction\" if metric != 'r2' else \"increase\"\n",
    "    print(f\"{metric.upper()}: {imp:+.2f}% {direction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize metric comparison\n",
    "metrics = ['MAE', 'MSE', 'RMSE', 'R²']\n",
    "linear_values = [metrics_linear['mae'], metrics_linear['mse'], metrics_linear['rmse'], metrics_linear['r2']]\n",
    "poly_values = [metrics_poly['mae'], metrics_poly['mse'], metrics_poly['rmse'], metrics_poly['r2']]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bars1 = ax.bar(x - width/2, linear_values, width, label='Linear', alpha=0.8)\n",
    "bars2 = ax.bar(x + width/2, poly_values, width, label='Polynomial', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Metrics')\n",
    "ax.set_ylabel('Values')\n",
    "ax.set_title('Model Performance Comparison')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "def add_value_labels(bars):\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.3f}',\n",
    "                   xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                   xytext=(0, 3),  # 3 points vertical offset\n",
    "                   textcoords=\"offset points\",\n",
    "                   ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "add_value_labels(bars1)\n",
    "add_value_labels(bars2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/figures/notebook_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. What We Learned\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Linear Baseline**: The linear regression model provides a solid baseline for predicting student scores based on study hours.\n",
    "\n",
    "2. **Polynomial Enhancement**: Adding polynomial features can capture non-linear relationships in the data, potentially improving prediction accuracy.\n",
    "\n",
    "3. **Bias-Variance Tradeoff**: Higher polynomial degrees may improve training performance but risk overfitting. Cross-validation helps select the optimal degree.\n",
    "\n",
    "4. **Feature Importance**: Study hours typically show the strongest correlation with final scores, but other factors like sleep, attendance, and participation can also be significant.\n",
    "\n",
    "### Recommendations:\n",
    "\n",
    "- **Model Selection**: Use cross-validation to select the optimal polynomial degree\n",
    "- **Feature Engineering**: Consider interactions between features and domain-specific transformations\n",
    "- **Data Collection**: Gather more diverse features that might influence student performance\n",
    "- **Regularization**: For higher-degree polynomials, consider Ridge or Lasso regression to prevent overfitting\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Collect more data points to improve model robustness\n",
    "- Experiment with other algorithms (Random Forest, Gradient Boosting, Neural Networks)\n",
    "- Perform feature selection to identify the most important predictors\n",
    "- Validate the model on completely unseen data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}